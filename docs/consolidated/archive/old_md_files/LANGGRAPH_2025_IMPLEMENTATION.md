# LangGraph 2025 Implementation Guide

## ðŸš€ **Overview**

The Climate Economy Assistant has been fully upgraded to **LangGraph 2025 execution patterns** with comprehensive support for `.ainvoke()`, `.astream()`, user steering, and human-in-the-loop workflows.

## ðŸŽ¯ **Key LangGraph 2025 Features Implemented**

### **âœ… Execution Patterns**
- **`.ainvoke()`** - Full response execution for complete agent workflows
- **`.astream()`** - Real-time streaming for responsive user experiences
- **Input Format Support** - All LangGraph 2025 input formats supported
- **Output Format Compliance** - Proper message lists and custom state fields

### **âœ… User Steering & Human-in-the-Loop**
- **4 Journey Stages** - Discovery â†’ Strategy â†’ Action Planning â†’ Implementation
- **6 User Steering Tools** - Interactive decision checkpoints throughout the journey
- **Progress Tracking** - Real-time milestone and decision tracking
- **Collaborative Control** - User actively guides their career development

### **âœ… Advanced State Management**
- **Custom State Schema** - Extended `ClimateAgentState` with user steering fields
- **Concurrent-Safe Updates** - Using `operator.add` for thread-safe state changes
- **Enhanced Intelligence** - Quality metrics, confidence scoring, and performance tracking

## ðŸ“Š **Implementation Architecture**

### **Backend (Python) - LangGraph 2025 Patterns**

#### **Enhanced Supervisor Endpoint**
```python
# backend/main.py - supervisor-chat endpoint

@app.post("/api/v1/supervisor-chat")
async def supervisor_chat_endpoint(request: Dict[str, Any], background_tasks: BackgroundTasks):
    """
    Enhanced supervisor chat endpoint using LangGraph 2025 execution patterns
    
    Supports:
    - .ainvoke() for full responses
    - .astream() for streaming responses
    - All LangGraph 2025 input formats
    - Custom state schema with user steering
    """
    
    # Create LangGraph 2025 compatible state
    initial_state = await _create_langgraph_2025_state(
        message=message,
        user_id=user_id,
        conversation_id=conversation_id,
        context=context,
        metadata=metadata
    )
    
    # LangGraph 2025 Execution Patterns
    if stream:
        # Use .astream() for streaming responses
        return await _handle_streaming_response(workflow, initial_state, ...)
    else:
        # Use .ainvoke() for full responses
        result = await workflow.ainvoke(initial_state)
        return _format_langgraph_2025_response(result)
```

#### **Input Format Support**
```python
async def _create_langgraph_2025_state(message, user_id, conversation_id, context, metadata):
    """
    Create LangGraph 2025 compatible state from various input formats
    
    Supports:
    - String input: {"messages": "Hello"}
    - Message dict: {"messages": {"role": "user", "content": "Hello"}}
    - Message list: {"messages": [{"role": "user", "content": "Hello"}]}
    - Custom state: All fields from ClimateAgentState
    """
    
    # Convert message to proper LangChain message format
    if isinstance(message, str):
        messages = [HumanMessage(content=message)]
    elif isinstance(message, dict):
        if message.get("role") == "user":
            messages = [HumanMessage(content=message.get("content", ""))]
    elif isinstance(message, list):
        messages = [HumanMessage(content=msg.get("content")) for msg in message]
    
    # Create enhanced state with user steering capabilities
    return ClimateAgentState(
        messages=messages,
        user_id=user_id,
        conversation_id=conversation_id,
        # User steering state (LangGraph 2025 human-in-the-loop)
        user_journey_stage=context.get("journey_stage", "discovery"),
        career_milestones=[],
        user_decisions=[],
        awaiting_user_input=False,
        # ... (full state configuration)
    )
```

#### **Streaming Implementation**
```python
async def _handle_streaming_response(workflow, initial_state, ...):
    """
    Handle streaming response using .astream() - LangGraph 2025 pattern
    """
    async def generate_stream():
        # Use .astream() for incremental streaming - LangGraph 2025
        async for chunk in workflow.astream(initial_state):
            if isinstance(chunk, dict):
                # Process different chunk types
                if "messages" in chunk:
                    # Stream new content incrementally
                    yield f"data: {json.dumps({'type': 'content', 'content': new_content})}\n\n"
                
                # User steering checkpoints
                if chunk.get("awaiting_user_input"):
                    yield f"data: {json.dumps({'type': 'user_input_needed', 'context': chunk.get('decision_context')})}\n\n"
                
                # Progress updates
                if "career_milestones" in chunk:
                    yield f"data: {json.dumps({'type': 'milestone', 'milestone': milestone})}\n\n"
    
    return StreamingResponse(generate_stream(), media_type="text/event-stream")
```

### **Enhanced Interactive Chat Endpoint**
```python
# backend/api/endpoints/interactive_chat.py

@router.post("/langgraph-chat")
async def langgraph_chat(request: ChatRequest, background_tasks: BackgroundTasks):
    """
    Enhanced chat using LangGraph 2025 multi-agent system with full streaming support
    """
    
    # Create LangGraph 2025 compatible state
    initial_state = await _create_interactive_chat_state(...)
    
    # LangGraph 2025 Execution Patterns
    if stream:
        return await _handle_interactive_streaming_response(workflow, initial_state, ...)
    else:
        result = await workflow.ainvoke(initial_state)
        return _format_interactive_chat_response(result, conversation_id)
```

### **Frontend (TypeScript) - Enhanced API Integration**

#### **Supervisor Chat with Streaming**
```typescript
// app/api/v1/supervisor-chat/route.ts

export async function POST(request: NextRequest) {
    // Parse request body with LangGraph 2025 input format support
    const {
        message,
        conversation_id,
        context = {},
        metadata = {},
        stream = false,
        user_journey_stage = "discovery",
        user_preferences = {},
        pathway_options = null,
        goals_validated = false,
        pathway_chosen = false,
        action_plan_approved = false
    } = body;

    // Prepare enhanced request with LangGraph 2025 user steering support
    const backendRequest = {
        message,
        user_id: user.id,
        conversation_id,
        stream, // LangGraph 2025 streaming flag
        context: {
            ...context,
            journey_stage: user_journey_stage,
            user_preferences,
            pathway_options,
            goals_validated,
            pathway_chosen,
            plan_approved: action_plan_approved,
            control_level: context.control_level || "collaborative"
        },
        metadata: {
            ...metadata,
            langgraph_2025: true,
            user_steering_enabled: true,
        }
    };

    // Handle streaming response if requested
    if (stream && backendResponse.ok) {
        // Enhanced stream processing for LangGraph 2025
        const processedStream = enhanceStreamingChunks(backendResponse.body);
        
        return new NextResponse(processedStream, {
            headers: {
                'Content-Type': 'text/event-stream',
                'X-LangGraph-2025': 'true',
                'X-User-Steering': 'enabled'
            }
        });
    }
}
```

#### **Interactive Chat with User Steering**
```typescript
// app/api/v1/interactive-chat/route.ts

export async function POST(request: NextRequest) {
    // Parse request body with LangGraph 2025 input format support
    const {
        query,
        context = {},
        stream = false,
        user_journey_stage = "discovery",
        user_preferences = {},
        awaiting_user_input = false,
        decision_context = null
    } = body;

    // Enhanced request with LangGraph 2025 support
    const backendRequest = {
        message: query,
        stream, // LangGraph 2025 streaming flag
        metadata: {
            journey_stage: user_journey_stage,
            user_preferences,
            awaiting_user_input,
            decision_context,
            langgraph_2025: true,
            interactive_chat: true,
            user_steering_enabled: true
        }
    };
}
```

## ðŸŽ¯ **User Steering Implementation**

### **User Journey Stages**
```python
class UserJourneyStage:
    DISCOVERY = "discovery"        # Understanding goals and identity
    STRATEGY = "strategy"          # Exploring pathways and options
    ACTION_PLANNING = "action_planning"  # Creating detailed action plans
    IMPLEMENTATION = "implementation"    # Ongoing support and guidance
```

### **User Steering Tools (Human-in-the-Loop)**
```python
# 6 Interactive decision checkpoints

@tool
def career_milestone_checkpoint(current_progress: str, next_options: List[str], user_preferences: Dict):
    """
    Present progress review with user direction - LangGraph interrupt() pattern
    """
    return interrupt({
        "type": "user_input_needed",
        "checkpoint_type": "milestone_review",
        "context": {
            "progress": current_progress,
            "options": next_options,
            "preferences": user_preferences
        }
    })

@tool
def pathway_selection_tool(pathway_options: List[Dict], user_background: Dict):
    """
    Present career pathway options for user selection
    """
    return interrupt({
        "type": "pathway_selection",
        "options": pathway_options,
        "user_context": user_background
    })

@tool
def skills_validation_checkpoint(current_skills: List[str], target_skills: List[str]):
    """
    Validate skills assessment with user input
    """
    return interrupt({
        "type": "skills_validation",
        "current": current_skills,
        "target": target_skills,
        "gaps": skill_gaps
    })

@tool
def goals_confirmation_tool(identified_goals: List[str], timeline: str):
    """
    Confirm career goals before proceeding
    """
    return interrupt({
        "type": "goals_confirmation",
        "goals": identified_goals,
        "timeline": timeline
    })

@tool
def action_plan_approval_tool(proposed_actions: List[Dict], timeline: Dict):
    """
    Get user approval for action plans
    """
    return interrupt({
        "type": "action_plan_approval",
        "actions": proposed_actions,
        "timeline": timeline
    })

@tool
def satisfaction_checkpoint_tool(session_summary: str, resources_provided: List[str]):
    """
    Gather user feedback and satisfaction
    """
    return interrupt({
        "type": "satisfaction_check",
        "summary": session_summary,
        "resources": resources_provided
    })
```

### **Enhanced State Schema**
```python
class ClimateAgentState(MessagesState):
    """Enhanced state with user steering and collaborative decision-making capabilities"""
    
    # USER STEERING AND COLLABORATION STATE
    user_journey_stage: str = "discovery"
    career_milestones: Annotated[List[Dict[str, Any]], operator.add] = []
    user_decisions: Annotated[List[Dict[str, Any]], operator.add] = []
    pathway_options: Optional[Dict[str, Any]] = None
    user_preferences: Optional[Dict[str, Any]] = None
    next_decision_point: Optional[str] = None
    user_control_level: str = "collaborative"
    
    # PROGRESS TRACKING AND VALIDATION
    goals_validated: bool = False
    skills_assessment_complete: bool = False
    pathway_chosen: bool = False
    action_plan_approved: bool = False
    implementation_started: bool = False
    
    # USER FEEDBACK AND STEERING
    awaiting_user_input: bool = False
    input_type_needed: Optional[str] = None
    decision_context: Optional[Dict[str, Any]] = None
    user_satisfaction_check: bool = False
    course_correction_needed: bool = False
    
    # COLLABORATIVE WORKFLOW STATE
    checkpoint_data: Optional[Dict[str, Any]] = None
    approved_actions: Annotated[List[str], operator.add] = []
    pending_approvals: Annotated[List[Dict[str, Any]], operator.add] = []
    user_modifications: Annotated[List[Dict[str, Any]], operator.add] = []
```

## ðŸ›  **Enhanced Workflow Architecture**

### **UserSteeringCoordinator**
```python
class UserSteeringCoordinator:
    """
    Manages user steering through the four career journey stages
    """
    
    async def create_decision_point(self, state: ClimateAgentState, decision_type: str, context: Dict, options: List[Dict]):
        """
        Create user decision checkpoint using LangGraph interrupt() pattern
        """
        return {
            "awaiting_user_input": True,
            "input_type_needed": decision_type,
            "decision_context": {
                "type": decision_type,
                "context": context,
                "options": options,
                "stage": state.user_journey_stage,
                "progress": self._calculate_progress(state)
            }
        }
```

### **Enhanced Supervisor Handler**
```python
async def supervisor_handler(state: ClimateAgentState) -> Dict[str, Any]:
    """
    Enhanced supervisor with stage-based processing and user steering
    """
    
    # Initialize user steering coordinator
    steering_coordinator = UserSteeringCoordinator()
    
    # Stage-based processing
    if state.user_journey_stage == "discovery":
        return await handle_discovery_stage(state, user_message, steering_coordinator)
    elif state.user_journey_stage == "strategy":
        return await handle_strategy_stage(state, user_message, steering_coordinator)
    elif state.user_journey_stage == "action_planning":
        return await handle_action_planning_stage(state, user_message, steering_coordinator)
    elif state.user_journey_stage == "implementation":
        return await handle_implementation_stage(state, user_message, steering_coordinator)
```

## ðŸ“Š **Output Format Compliance**

### **LangGraph 2025 Response Format**
```python
def _format_langgraph_2025_response(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Format workflow result to LangGraph 2025 output format
    
    Output format includes:
    - messages: List of all messages
    - content: Assistant response content  
    - specialist: Current specialist
    - Custom state fields from ClimateAgentState
    """
    
    return {
        # Core response
        "success": True,
        "content": response_content,
        "messages": result.get("messages", []),
        
        # Agent information
        "specialist": specialist,
        "tools_used": result.get("tools_used", []),
        "next_actions": result.get("next_actions", []),
        
        # Workflow state
        "workflow_state": result.get("workflow_state", "completed"),
        "intelligence_level": result.get("intelligence_level", "developing"),
        "confidence_score": result.get("confidence_score", 0.0),
        
        # User steering state (LangGraph 2025 human-in-the-loop)
        "user_journey_stage": result.get("user_journey_stage", "discovery"),
        "career_milestones": result.get("career_milestones", []),
        "user_decisions": result.get("user_decisions", []),
        "awaiting_user_input": result.get("awaiting_user_input", False),
        "input_type_needed": result.get("input_type_needed"),
        "decision_context": result.get("decision_context"),
        
        # Progress tracking
        "goals_validated": result.get("goals_validated", False),
        "pathway_chosen": result.get("pathway_chosen", False),
        "action_plan_approved": result.get("action_plan_approved", False),
        
        # Structured response (optional for custom structured output)
        "structured_response": result.get("structured_response"),
        
        # Metadata
        "metadata": {
            "execution_time": result.get("last_update_time"),
            "enhanced_intelligence": True,
            "user_steering_enabled": True,
            "langgraph_2025": True
        }
    }
```

## ðŸš€ **Usage Examples**

### **Basic .ainvoke() Usage**
```python
# Full response execution
result = await climate_supervisor_graph.ainvoke({
    "messages": [HumanMessage(content="I'm a veteran looking for climate jobs")],
    "user_id": "user123",
    "conversation_id": "conv456", 
    "user_journey_stage": "discovery"
})

print(result["content"])  # Assistant response
print(result["specialist"])  # "marcus" (veteran specialist)
print(result["user_steering"]["journey_stage"])  # Current stage
```

### **Streaming .astream() Usage**
```python
# Real-time streaming execution
async for chunk in climate_supervisor_graph.astream(initial_state):
    if chunk.get("awaiting_user_input"):
        # User steering checkpoint reached
        decision_context = chunk["decision_context"]
        await present_decision_to_user(decision_context)
    
    if "messages" in chunk:
        # New content available
        content = chunk["messages"][-1].content
        await stream_to_frontend(content)
```

### **Frontend Streaming Integration**
```typescript
// Frontend streaming with user steering
const response = await fetch('/api/v1/supervisor-chat', {
    method: 'POST',
    body: JSON.stringify({
        message: "Help me transition to climate work",
        stream: true,
        user_journey_stage: "discovery",
        user_preferences: { location: "Boston", focus: "renewable_energy" }
    })
});

const reader = response.body?.getReader();
while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    const chunk = new TextDecoder().decode(value);
    if (chunk.startsWith('data: ')) {
        const data = JSON.parse(chunk.slice(6));
        
        if (data.type === 'user_input_needed') {
            // Show decision modal to user
            showUserSteeringModal(data.context);
        } else if (data.type === 'content') {
            // Stream content to chat
            appendToChat(data.content);
        } else if (data.type === 'milestone') {
            // Update progress indicator
            updateProgressBar(data.milestone);
        }
    }
}
```

## ðŸŽ¯ **Key Benefits Achieved**

### **âœ… Full LangGraph 2025 Compliance**
- **Execution Patterns** - Proper `.ainvoke()` and `.astream()` implementation
- **Input Formats** - Support for all LangGraph 2025 input types
- **Output Formats** - Compliant message lists and custom state fields
- **State Management** - Concurrent-safe updates with proper annotations

### **âœ… Advanced User Experience**
- **Real-time Streaming** - Responsive user interactions with progress updates
- **User Steering** - Active user control over their career journey
- **Human-in-the-Loop** - Collaborative decision-making at key checkpoints
- **Progress Tracking** - Visual milestone and stage progression

### **âœ… Enterprise-Ready Architecture**
- **Scalability** - Auto-scaling workflows with performance optimization
- **Reliability** - Enhanced error handling and recovery mechanisms
- **Monitoring** - Comprehensive analytics and performance tracking
- **Security** - Proper authentication and data protection

### **âœ… Climate Career Specialization**
- **Expert Routing** - Intelligent specialist assignment (Jasmine, Marcus, Liv, Miguel, Alex)
- **Community Support** - Targeted assistance for veterans, international professionals, EJ communities
- **Massachusetts Focus** - Gateway Cities integration and local resource connections
- **Skills Translation** - Military, international, and traditional skill mapping

This implementation represents a complete, production-ready LangGraph 2025 system that delivers exceptional user experiences while maintaining the highest standards of technical excellence and climate career specialization. 