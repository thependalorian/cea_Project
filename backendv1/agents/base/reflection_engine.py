"""
Reflection Engine for Agent Self-Assessment

Following rule #12: Complete code verification with proper reflection capabilities
Following rule #15: Include comprehensive error handling

This module provides self-assessment and improvement capabilities for agents.
Location: backendv1/agents/base/reflection_engine.py
"""

from typing import Dict, Any, List, Optional
from datetime import datetime

from backendv1.utils.logger import setup_logger
from backendv1.adapters.openai_adapter import OpenAIAdapter

logger = setup_logger("reflection_engine")


class ReflectionEngine:
    """
    Self-reflection and improvement engine for agents

    Following rule #12: Complete code verification with proper typing
    """

    def __init__(self, agent_name: str):
        """Initialize reflection engine for an agent"""
        self.agent_name = agent_name
        self.reflection_history: List[Dict[str, Any]] = []
        self.performance_metrics: Dict[str, float] = {}
        self.openai_adapter = OpenAIAdapter()
        logger.info(f"ðŸ” Reflection engine initialized for {agent_name}")

    async def reflect_on_interaction(self, interaction: Dict[str, Any]) -> Dict[str, Any]:
        """
        Reflect on a completed interaction

        Args:
            interaction: Interaction data to reflect on

        Returns:
            Dict[str, Any]: Reflection results
        """
        try:
            # Extract relevant information for reflection
            interaction_type = interaction.get("type", "unknown")
            interaction_id = interaction.get("id", str(len(self.reflection_history)))

            # For quality analysis interactions, extract metrics directly
            if interaction_type == "quality_analysis" and "metrics" in interaction:
                quality_score = interaction["metrics"].get("overall_quality", 0.8)
            else:
                # For other interactions, calculate quality score
                quality_score = await self._assess_quality(interaction)

            # Identify strengths and improvements
            strengths, improvements = await self._identify_strengths_and_improvements(interaction)

            # Extract lessons learned
            lessons_learned = await self._extract_lessons(interaction, strengths, improvements)

            # Create reflection record
            reflection = {
                "interaction_id": interaction_id,
                "timestamp": datetime.utcnow().isoformat(),
                "agent": self.agent_name,
                "quality_score": quality_score,
                "improvements": improvements,
                "strengths": strengths,
                "lessons_learned": lessons_learned,
            }

            self.reflection_history.append(reflection)
            logger.debug(f"ðŸ” Reflection completed for {self.agent_name}")

            # Update performance metrics
            self.performance_metrics.update(
                {
                    "last_quality_score": quality_score,
                    "reflection_count": len(self.reflection_history),
                }
            )

            return reflection

        except Exception as e:
            logger.error(f"Error during reflection: {e}")
            return {}

    async def _assess_quality(self, interaction: Dict[str, Any]) -> float:
        """Assess quality of an interaction"""
        try:
            # Extract response if available
            response_content = ""
            if "response" in interaction:
                response_content = interaction["response"]
            elif "content" in interaction:
                response_content = interaction["content"]

            if not response_content:
                return 0.8  # Default score if no content

            # Create quality assessment prompt
            assessment_prompt = f"""
You are an expert at evaluating AI responses. Analyze this response:

---
{response_content}
---

Rate the overall quality on a scale from 0.0 to 1.0, where 1.0 is perfect.
Consider clarity, helpfulness, accuracy, and relevance.
Output only the numeric score without any explanation.
"""

            # Get quality assessment from OpenAI
            assessment_data = await self.openai_adapter.complete(
                assessment_prompt, temperature=0.1, max_tokens=10
            )

            # Extract numeric score
            try:
                score_text = assessment_data.get("content", "0.8").strip()
                score = float(score_text)
                # Ensure score is in valid range
                score = max(0.0, min(1.0, score))
                return score
            except ValueError:
                logger.warning(f"Could not parse quality score: {assessment_data.get('content')}")
                return 0.8

        except Exception as e:
            logger.error(f"Error assessing quality: {e}")
            return 0.8

    async def _identify_strengths_and_improvements(
        self, interaction: Dict[str, Any]
    ) -> tuple[List[str], List[str]]:
        """Identify strengths and improvement areas"""
        try:
            # Extract response if available
            response_content = ""
            if "response" in interaction:
                response_content = interaction["response"]
            elif "content" in interaction:
                response_content = interaction["content"]

            if not response_content:
                return (
                    ["Insufficient data for strength analysis"],
                    ["Insufficient data for improvement analysis"],
                )

            # Create analysis prompt
            analysis_prompt = f"""
You are an expert at analyzing AI responses. Review this response:

---
{response_content}
---

Identify:
1. Three key strengths of this response
2. Three areas for improvement

Format your response as a JSON object with two arrays: "strengths" and "improvements".
Each array should contain 1-3 brief, specific points as strings.
"""

            # Get analysis from OpenAI
            analysis_data = await self.openai_adapter.complete(
                analysis_prompt,
                temperature=0.2,
                max_tokens=500,
                response_format={"type": "json_object"},
            )

            # Extract strengths and improvements
            try:
                content = analysis_data.get("content", "{}")
                import json

                analysis = json.loads(content)

                strengths = analysis.get("strengths", [])
                improvements = analysis.get("improvements", [])

                # Ensure we have at least some data
                if not strengths:
                    strengths = ["Clear communication", "Helpful information"]
                if not improvements:
                    improvements = ["Could be more concise", "Could add more specific examples"]

                return strengths, improvements

            except Exception as parse_error:
                logger.warning(f"Could not parse strengths/improvements: {parse_error}")
                return (
                    ["Clear communication", "Helpful information"],
                    ["Could be more concise", "Could add more specific examples"],
                )

        except Exception as e:
            logger.error(f"Error identifying strengths/improvements: {e}")
            return (
                ["Clear communication", "Helpful information"],
                ["Could be more concise", "Could add more specific examples"],
            )

    async def _extract_lessons(
        self, interaction: Dict[str, Any], strengths: List[str], improvements: List[str]
    ) -> List[str]:
        """Extract lessons learned from interaction"""
        try:
            # Basic lessons from strengths and improvements
            lessons = []

            # Add lessons based on strengths to reinforce
            for strength in strengths[:2]:  # Use top 2 strengths
                lessons.append(f"Continue to {strength.lower()}")

            # Add lessons based on improvements
            for improvement in improvements[:2]:  # Use top 2 improvements
                if improvement.lower().startswith("could "):
                    lesson = improvement.replace("Could ", "Should ")
                    lessons.append(lesson)
                else:
                    lessons.append(f"Should improve: {improvement.lower()}")

            return lessons

        except Exception as e:
            logger.error(f"Error extracting lessons: {e}")
            return ["Continue providing clear information", "Should be more concise"]

    async def assess_performance(self) -> Dict[str, float]:
        """
        Assess overall agent performance

        Returns:
            Dict[str, float]: Performance metrics
        """
        try:
            # Calculate metrics based on reflection history
            if not self.reflection_history:
                return {
                    "response_quality": 0.8,
                    "user_satisfaction": 0.75,
                    "task_completion": 0.85,
                    "efficiency": 0.9,
                }

            # Calculate average quality score
            quality_scores = [
                r.get("quality_score", 0) for r in self.reflection_history if "quality_score" in r
            ]
            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0.8

            # Calculate improvement trend (positive if recent scores are higher)
            if len(quality_scores) >= 5:
                recent_avg = sum(quality_scores[-5:]) / 5
                earlier_avg = (
                    sum(quality_scores[:-5]) / (len(quality_scores) - 5)
                    if len(quality_scores) > 5
                    else 0.7
                )
                improvement_trend = (recent_avg - earlier_avg) + 0.5  # Normalize around 0.5
                improvement_trend = max(0.1, min(1.0, improvement_trend))  # Ensure range 0.1-1.0
            else:
                improvement_trend = 0.5

            # Generate metrics
            metrics = {
                "response_quality": avg_quality,
                "user_satisfaction": 0.9 * avg_quality,  # Estimated based on quality
                "task_completion": min(
                    1.0, 0.75 + 0.25 * avg_quality
                ),  # Base completion rate plus quality factor
                "efficiency": 0.7
                + 0.2 * improvement_trend,  # Base efficiency plus improvement trend
                "improvement_trend": improvement_trend,
            }

            self.performance_metrics.update(metrics)
            logger.debug(f"ðŸ” Performance assessed for {self.agent_name}")
            return metrics

        except Exception as e:
            logger.error(f"Error assessing performance: {e}")
            return {
                "response_quality": 0.8,
                "user_satisfaction": 0.75,
                "task_completion": 0.85,
                "efficiency": 0.9,
            }

    async def generate_improvement_plan(self) -> List[str]:
        """
        Generate improvement recommendations

        Returns:
            List[str]: Improvement recommendations
        """
        try:
            # If we have reflection history, analyze patterns
            if len(self.reflection_history) >= 3:
                # Collect all improvements from recent reflections
                all_improvements = []
                for reflection in self.reflection_history[
                    -10:
                ]:  # Use up to 10 most recent reflections
                    all_improvements.extend(reflection.get("improvements", []))

                # Count frequency of improvement themes
                improvement_counts = {}
                for improvement in all_improvements:
                    key = improvement.lower()
                    improvement_counts[key] = improvement_counts.get(key, 0) + 1

                # Sort by frequency to find common improvement needs
                sorted_improvements = sorted(
                    improvement_counts.items(), key=lambda x: x[1], reverse=True
                )

                # Generate targeted recommendations based on patterns
                if sorted_improvements:
                    recommendations = []
                    for improvement, count in sorted_improvements[:3]:
                        if "concise" in improvement or "brief" in improvement:
                            recommendations.append(
                                "Optimize response length and structure for better readability"
                            )
                        elif "example" in improvement or "specific" in improvement:
                            recommendations.append(
                                "Include more concrete examples and specific actionable steps"
                            )
                        elif "personal" in improvement or "tailor" in improvement:
                            recommendations.append(
                                "Enhance personalization by better referencing user context"
                            )
                        else:
                            recommendations.append(f"Focus on improvement area: {improvement}")

                    # Add general improvement if needed to reach 3 recommendations
                    while len(recommendations) < 3:
                        recommendations.append(
                            "Continue monitoring response quality and user satisfaction"
                        )

                    return recommendations

            # Default improvement plan if insufficient history
            return [
                "Enhance response personalization",
                "Improve source citation accuracy",
                "Increase empathy in responses",
            ]

        except Exception as e:
            logger.error(f"Error generating improvement plan: {e}")
            return [
                "Enhance response personalization",
                "Improve source citation accuracy",
                "Increase empathy in responses",
            ]


# Export main class
__all__ = ["ReflectionEngine"]
